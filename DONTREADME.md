# pun

## Goals

### Identifiability

> Does `pun` use human surveys to evaluate identifiability?

No. Human surveys would be ideal for measuring how recognizable puns are, but that's way too time-consuming.

Instead of trying to directly measure the identifiability of the final puns, `pun` enforces process controls that make identifiable puns more likely.

### Relevance

> What metric is used to evaluate the relevance of puns generated by `pun`?

`pun` uses cosine similarity to check how relevant the puns are.

### Quantity

> Does `pun` aim to generate more than one pun?

Yep! The tool tries to give you multiple puns for your content. This is super helpful when you're jumping into comment threads. You'll have different puns ready for different replies without recycling the same one.

### Latency

> What's the target response time for `pun`?

The target response time is ten seconds maximum. "[1.0 second is about the limit for the user's flow of thought to stay uninterrupted](https://www.nngroup.com/articles/response-times-3-important-limits/#:~:text=1.0%20second%20is%20about%20the%20limit%20for%20the%20user's%20flow%20of%20thought%20to%20stay%20uninterrupted)."

When you fire up the `pun` command, it kickstarts a background server, so you won't have to wait as long next time.

> What's the target response time while the background server is running?

The target response time is one second when the background server is alive. "[10 seconds is about the limit for keeping the user's attention focused on the dialogue.](https://www.nngroup.com/articles/response-times-3-important-limits/#:~:text=10%20seconds%20is%20about%20the%20limit%20for%20keeping%20the%20user%27s%20attention%20focused%20on%20the%20dialogue.)"

## Vocabulary

> Does `pun` limit the vocabulary it uses?

Yep. This keeps the tool from creating puns with weird obscure words terms that would fly over most people's heads.

> Does the vocabulary rely on a dictionary?

Yes. `pun` draws its vocabulary from English Wiktionary entries.

> Does the vocabulary rely on Wikipedia?

Nope. Wikipedia terms minus English Wiktionary English terms would mostly just end up with specialized named entities.

> Does `pun` filter vocabulary by word frequency?

Nah. Word frequency alone doesn't determine how identifiable a term is. For example, "ungoogleable" barely registers on frequency lists, but everyone knows what it means because it's derived from "Google".

Plus, frequency filtering gets messy with phrases, which can show up in all sorts of variations and are a pain to count.

Instead, `pun` uses large language models to score how recognizable each word is, then filters based on those scores.

> Can the recognizability score be negative?

No, because it's a percentage.

Specifically, it's the percentage of Americans 10 years or older who know the most frequently used meaning of each phrase.

- "Americans" pins it to a clear population, avoiding wishy-washy concepts like "native speakers" that are open to interpretation.

- "10 years or older" filters out babies, making it easier to sanity-check the model output, as super common words should hit near 100%.

- "Most frequently used meaning" focuses on the idiomatic meaning of the phrase, because puns typically play off these established meanings.

> Is the recognizability score an integer?

Nah, it's a double. Doubles allow finer ordering.

> Does `pun` use local LLMs for recognizability scoring?

Nah. Remote LLMs give state-of-the-art results.

> What model does `pun` use for recognizability scoring?

[Gemini 2.0 Flash](https://deepmind.google/technologies/gemini/flash) is currently used, and here's why:

- It seems to pretty accurate estimates.

- It's cheap.

- Rate limits are high.

> Does `pun` calculate recognizability scores on the fly?

No. The scores are precomputed because:

- API calls to remote LLMs cost actual cash.

- Running scoring takes time.

- Random API failures would break your pun flow.

> Does `pun` evaluate all phrases in one giant request?

Nah. That ain't happening because:

- Shoving all phrases in would blow past max output token lengths.

- longer outputs tend to exhibit decreased quality

So, the phrases are split into chunks and evaluated separately.

> Does `pun` use CSV for storing recognizability scores?

Nope. CSV is not ideal here because it lacks a proper key-value structure, which could lead to duplicate phrase entries.

> Does `pun` use JSON for storing recognizability scores?

Nah. `pun` uses EDN instead of JSON because:

- It's natively supported in Clojure, the backend language

- It's a bit more concise than JSON since it doesn't require commas between entries

> Are the precomputed recognizability scores committed to this repository?

Nah. These scores are generated data, not source code.

> Are the precomputed recognizability scores included in automated releases?

No way. The scoring process occasionally needs manual babysitting. API calls might fail, models might return garbage, or other random stuff can go wrong. Since it costs real money to run these LLM calls, I don't want to blindly retry in an automated pipeline. It's the kind of process I want to run manually, check the results, and then commit when I'm satisfied.

> Are the recognizability scores guaranteed to be reproducible?

No way. Reproducibility is not guaranteed for the following reasons:

- Even with temperature set to zero, many LLMs still aren't deterministic.

- Remote model providers may update their models.

## Substitution

> Can `pun` use homophones for substitution?

Yep! `pun` can totally use homophones for substitution. But it's not limited to perfect homophones. The tool works with words that share significant phonetic similarity too, which gives it way more flexibility when cranking out puns.

> What metric is used to calculate phonetic similarity?

`pun` uses Levenshtein distance on International Phonetic Alphabet (IPA) representations. For substitution jokes to land, the audience has to recognize the original phrase being referenced. The tool converts words to their IPA representation and then calculates the Levenshtein distance between them to figure out how phonetically similar they are. This approach makes sure substitutions keep enough phonetic similarity to keep the puns identifiable.

> What library does `pun` use for converting English text to IPA?

`pun` uses [`epitran`](https://github.com/dmort27/epitran). I tried [`eng_to_ipa`](https://github.com/mphilli/English-to-IPA), [`espeak-ng`](https://github.com/espeak-ng/espeak-ng), and [`g2p`](https://github.com/roedoejet/g2p) too, but they weren't accurate enough.

> Does `pun` use dictionaries for converting English text to IPA?

Nope. Using dictionaries alone runs into these problems:

- Dictionaries might miss words that are well-known but not super common, like "ungoogleable".

- Dictionary lookups struggle with how words change pronunciation in context, like how "the" sounds different before vowels versus consonants.

Sure, I could try to bolt dictionaries onto a conversion library for better accuracy, but that's a job for the IPA conversion library itself, not `pun`.

> Are the results sorted by Levenshtein distance?

Nope! Here's why:

- Phonetic similarity is just one piece of what makes a pun work

- Creating a weighting between different factors would be arbitrary

> Can `pun` generate ungrammatical puns?

Yep. That's because:

- Plenty of good puns break grammar rules on purpose.

- You can tweak the grammar yourself after getting the core wordplay idea, often needing to change more than just the substituted word to make it flow.

> Can `pun` generate multiple puns that differ only in the grammatical form of the substituted word?

Yep! These aren't filtered out because:

- Different forms might land better depending on where you're using the pun.

- Forcing you to mentally transform words wastes your brain power.
