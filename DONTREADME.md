# pun

## Goals

### Identifiability

> Does `pun` use human surveys to evaluate identifiability?

No. Human surveys would be ideal for measuring how recognizable puns are, but that's way too time-consuming.

Instead of trying to directly measure the identifiability of the final puns, `pun` enforces process controls that make identifiable puns more likely.

### Relevance

> What metric is used to evaluate the relevance of puns generated by `pun`?

`pun` uses cosine similarity to check how relevant the puns are.

### Quantity

> Does `pun` aim to generate more than one pun?

Yep! The tool tries to give you multiple puns for your content. This is super helpful when you're jumping into comment threads. You'll have a whole barrel of puns ready for different replies without recycling the same one.

### Latency

> What's the target response time for `pun`?

The target response time is ten seconds maximum. "[1.0 second is about the limit for the user's flow of thought to stay uninterrupted](https://www.nngroup.com/articles/response-times-3-important-limits/#:~:text=1.0%20second%20is%20about%20the%20limit%20for%20the%20user's%20flow%20of%20thought%20to%20stay%20uninterrupted)."

When you fire up the `pun` command, it kickstarts a background server, so you won't have to wait as long next time.

> What's the target response time while the background server is running?

The target response time is one second when the background server is alive. "[10 seconds is about the limit for keeping the user's attention focused on the dialogue.](https://www.nngroup.com/articles/response-times-3-important-limits/#:~:text=10%20seconds%20is%20about%20the%20limit%20for%20keeping%20the%20user%27s%20attention%20focused%20on%20the%20dialogue.)"

## Vocabulary

> Does `pun` limit the vocabulary it uses?

Yep. This keeps the tool from creating puns with weird obscure words terms that would fly over most people's heads.

> Does the vocabulary rely on a dictionary?

Yes. `pun` draws its vocabulary from English Wiktionary entries.

> Does the vocabulary rely on Wikipedia?

Nope. Wikipedia terms minus English Wiktionary English terms would mostly just end up with specialized named entities.

> Does `pun` filter vocabulary by word frequency?

Nah. Word frequency alone doesn't determine how identifiable a term is. For example, "ungoogleable" barely registers on frequency lists, but everyone knows what it means because it's derived from "Google".

Plus, frequency filtering gets messy with phrases, which can show up in all sorts of variations and are a pain to count.

Instead, `pun` uses large language models (LLMs) to score how recognizable each word is, then filters based on those scores.

> Can the recognizability score be negative?

No, because it's a percentage.

Specifically, it's the percentage of Americans 10 years or older who know the most frequently used meaning of each phrase.

- "Americans" pins it to a clear population, avoiding wishy-washy concepts like "native speakers" that are open to interpretation. Plus, for doing all that phonetic substitution stuff later, phonetic resources are easier to find for American English compared to, say, British English.

- "10 years or older" filters out babies, making it easier to sanity-check the model output, as super common words should hit near 100%.

- "Most frequently used meaning" focuses on the idiomatic meaning of the phrase, because puns typically play off these established meanings.

> Is the recognizability score an integer?

Nah, it's a double. Doubles allow finer ordering.

> Does `pun` use local LLMs for recognizability scoring?

Nah. Remote LLMs give state-of-the-art results.

> What model does `pun` use for recognizability scoring?

[Gemini 2.0 Flash](https://deepmind.google/technologies/gemini/flash) is currently used, and here's why:

- It seems to pretty accurate estimates.

- It's cheap.

- Rate limits are high.

> Does `pun` calculate recognizability scores on the fly?

No. The scores are precomputed because:

- API calls to remote LLMs cost actual cash.

- Running scoring takes time.

- Random API failures would break your pun flow.

> Does `pun` evaluate all phrases in one giant request?

Nah. That ain't happening because:

- Shoving all phrases in would blow past max output token lengths.

- longer outputs tend to exhibit decreased quality

So, the phrases are split into chunks and evaluated separately.

> Are the recognizability scores normalized across multiple runs?

Yes. Normalization makes the scores more consistent between different batches.

A single benchmark word is sufficient. The primary goal is to accurately assess recognizability around the threshold suitable for pun generation.

> What's the benchmark word?

The benchmark word is "touchstone". This word was chosen because it has the following characteristics:

- It is neither super common nor super obscure.

- It means "benchmark".

> What's the normalization formula?

It's piecewise:

$$
\bar{X} =
\begin{cases}
\frac{X \cdot \bar{B}}{B} & \text{if } X \leq B \\
100 - \frac{(100 - X)(100 - \bar{B})}{100 - B} & \text{if } X > B
\end{cases}
$$

where:

- $X$: The original score of a target word in the current run.

- $\bar{X}$: The normalized score of the target word.

- $B$: The score of the benchmark word in the current run.

- $\bar{B}$: The mean score of the benchmark word across all runs.

It is assumed that $B \neq 0$ and $B \neq 100$. If $B$ ever hits 0 or 100, that run gets tossed and re-done.

This piecewise approach ensures that scores of 0% and 100% remain unchanged, while scores near the benchmark are adjusted proportionally to the benchmark word's difference from its mean.

> Does the prompt include a sample answer?

Yep! The prompt's got a sample answer to nudge the LLM into spitting out the format I'm after.

> How many phrases are included in the sample answer?

Two phrases are included: "the" and "to".

Two phrases are plenty to show off a map structure. One lone phrase might get misread as a request for only one result.

[They are the two most frequent word forms](https://www.wordfrequency.info/samples/wordFrequency.xlsx), per the Corpus of Contemporary American English, which aligns with the target demographic of Americans 10 and up. These two words can confidently be assigned a recognizability score of 100.0%.

> Does `pun` use CSV for storing recognizability scores?

Nope. CSV is not ideal here because it lacks a proper key-value structure, which could lead to duplicate phrase entries.

> Does `pun` use JSON for storing recognizability scores?

Nah. `pun` uses EDN instead of JSON because:

- It's natively supported in Clojure, the backend language

- It's a bit more concise than JSON since it doesn't require commas between entries

> Are the precomputed recognizability scores committed to this repository?

Nah. These scores are generated data, not source code.

> Are the precomputed recognizability scores included in automated releases?

No way. The scoring process occasionally needs manual babysitting. API calls might fail, models might return garbage, or other random stuff can go wrong. Since it costs real money to run these LLM calls, I don't want to blindly retry in an automated pipeline. It's the kind of process I want to run manually, check the results, and then commit when I'm satisfied.

> Are the recognizability scores guaranteed to be reproducible?

No way. Reproducibility is not guaranteed for the following reasons:

- Even with temperature set to zero, many LLMs still aren't deterministic.

- Remote model providers may update their models.

## Relevant Phrase Identification

> What metric does `pun` use to determine the relevance of phrases to the input text?

`pun` uses cosine similarity. It gets one embedding for the whole input text, and embeddings for each vocabulary entry. If the cosine similarity between the input and a vocabulary entry is above a threshold, that entry is considered relevant.

> Must relevant phrases be present in the input text?

Nah. If `pun` only used phrases from the input, it'd be super limited, especially with short inputs.

> Does `pun` pick the embedding model that scores the highest on Semantic Textual Similarity (STS) tasks within the [Massive Text Embedding Benchmark](https://github.com/embeddings-benchmark/mteb) (MTEB)?

Nope. It doesn't just grab the top STS model from MTEB for a few reasons:

- Locality: Local models are preferred for performance and to avoid reliance on remote services.

- Security: `pun` skips models requiring remote code execution due to potential security vulnerabilities.

- Asymmetry: The relevant phrase identification task is asymmetric, comparing an embedding of the entire input text to embeddings of individual vocabulary entries, while STS tasks are typically symmetric, comparing sentences to sentences.

> Are the embeddings of vocabulary entries calculated on the fly?

Nope, those embeddings are precomputed. Calculating them every time would be way too slow.

## Phonetic Similarity Analysis

> Can `pun` use homophones for substitution?

Yep! `pun` can totally use homophones for substitution. But it's not limited to perfect homophones. The tool works with words that share significant phonetic similarity too, which gives it way more flexibility when cranking out puns.

> What metric is used to calculate phonetic similarity?

`pun` uses Levenshtein distance on International Phonetic Alphabet (IPA) representations. For substitution jokes to land, the audience has to recognize the original phrase being referenced. The tool converts words to their IPA representation and then calculates the Levenshtein distance between them to figure out how phonetically similar they are. This approach makes sure substitutions keep enough phonetic similarity to keep the puns identifiable.

> What library does `pun` use for converting English text to IPA?

`pun` uses [`epitran`](https://github.com/dmort27/epitran). I tried [`eng_to_ipa`](https://github.com/mphilli/English-to-IPA), [`espeak-ng`](https://github.com/espeak-ng/espeak-ng), and [`g2p`](https://github.com/roedoejet/g2p) too, but they weren't accurate enough.

> Does `pun` use dictionaries for converting English text to IPA?

Nope. Using dictionaries alone runs into these problems:

- Dictionaries might miss words that are well-known but not super common, like "ungoogleable".

- Dictionary lookups struggle with how words change pronunciation in context, like how "the" sounds different before vowels versus consonants.

Sure, I could try to bolt dictionaries onto a conversion library for better accuracy, but that's a job for the IPA conversion library itself, not `pun`.

## Substitution

> Are the results sorted by Levenshtein distance?

Nope! Here's why:

- Phonetic similarity is just one piece of what makes a pun work

- Creating a weighting between different factors would be arbitrary

> Can `pun` generate ungrammatical puns?

Yep. That's because:

- Plenty of good puns break grammar rules on purpose.

- You can tweak the grammar yourself after getting the core wordplay idea, often needing to change more than just the substituted word to make it flow.

> Can `pun` generate multiple puns that differ only in the grammatical form of the substituted word?

Yep! These aren't filtered out because:

- Different forms might land better depending on where you're using the pun.

- Forcing you to mentally transform words wastes your brain power.
